{"cells":[{"cell_type":"markdown","metadata":{"id":"GQ5fXK-ZhHJF"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-t28H5_hHJG"},"outputs":[],"source":["import re\n","import requests\n","from bs4 import BeautifulSoup\n","\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","from selenium.webdriver.common.by import By\n","from selenium.common.exceptions import NoSuchElementException\n","from urllib.parse import urlparse\n","\n","import csv\n","import os\n","import pandas as pd\n","\n","import time"]},{"cell_type":"markdown","metadata":{"id":"Pkltl9qDhHJH"},"source":["#### Executing Selenium"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"qtHIwqQxhHJI","executionInfo":{"status":"ok","timestamp":1750065711148,"user_tz":-420,"elapsed":22,"user":{"displayName":"Vinh Lê","userId":"10226406396810093297"}}},"outputs":[],"source":["def executing_selenium(url):\n","    chrome_options = Options()\n","    chrome_options.add_argument(\"--incognito\")\n","    chrome_options.add_argument(\"--window-size=1920x1080\")\n","    chrome_options.add_argument(\"--headless\")\n","    driver = webdriver.Chrome(options=chrome_options)\n","    driver.get(url)\n","    time.sleep(2)\n","    return driver"]},{"cell_type":"markdown","metadata":{"id":"R-1crbgrhHJI"},"source":["#### Xử lí url"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrUb_OVRhHJJ"},"outputs":[],"source":["def remove_last_slash(link):\n","    # Kiểm tra xem đường dẫn có kết thúc bằng dấu \"/\" không\n","    if link.endswith(\"/\"):\n","        # Nếu có, loại bỏ dấu \"/\" cuối cùng bằng cách cắt chuỗi từ đầu đến ký tự thứ hai từ cuối\n","        remove_link = link[:-1]\n","        return remove_link\n","    return link"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZY_3yAKhHJJ","outputId":"5fa0c7d0-f1a2-42be-829c-0dc3b4827fda"},"outputs":[{"name":"stdout","output_type":"stream","text":["may-tinh-bang\n"]}],"source":["def remove_first_slash(link):\n","    if link.startswith(\"/\"):\n","        remove_link = link[1:]\n","        return remove_link\n","    return link\n","\n","print(remove_first_slash(\"/may-tinh-bang\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dxucCOZAhHJK"},"outputs":[],"source":["def extract_path_and_remove_query_params(url):\n","    parsed_url = urlparse(url)\n","    path_without_query = parsed_url.path.split('?')[0]\n","    path_parts = path_without_query.split('/')\n","    category = path_parts[1]\n","    product_name = path_parts[2]\n","    return category, product_name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"brdHi_2ThHJK"},"outputs":[],"source":["def split_url(url):\n","    # Sử dụng biểu thức chính quy để tìm vị trí của kí tự số cuối cùng\n","    match = re.search(r'\\d+$', url)\n","    if match:\n","        # Nếu tìm thấy, lấy vị trí của kí tự số cuối cùng\n","        poi = match.start()\n","        # Tách chuỗi thành hai phần\n","        prefix = url[:poi]\n","        endfix = url[poi:]\n","        return prefix, endfix\n","    else:\n","        # Nếu không tìm thấy kí tự số cuối cùng, trả về None cho cả hai phần\n","        return url, \"\""]},{"cell_type":"markdown","metadata":{"id":"mZx_2R2HhHJL"},"source":["#### Check categories nào nằm trong danh sách crawl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UEMZHZGvhHJL"},"outputs":[],"source":["def check_keywords_in_link(link, keywords):\n","    keyword_pattern = re.compile('|'.join(keywords), re.IGNORECASE)\n","    return bool(keyword_pattern.search(link))"]},{"cell_type":"markdown","metadata":{"id":"T4hMkc2ThHJL"},"source":["#### Crawl category"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oon-coKAhHJL"},"outputs":[],"source":["def crawl_category(base_link, categories, remove_key_word):\n","    driver = executing_selenium(base_link)\n","    content = driver.page_source\n","    result = []\n","    soup = BeautifulSoup(content, \"html.parser\")\n","    menu = soup.find('ul', class_='main-menu')\n","    list_a =  menu.findChildren('a')\n","    base_link_remove_last_slash = remove_last_slash(base_link)\n","    for i in list_a:\n","        if not any (word in i.get('href') for word in remove_key_word) and i.get('href') != None and check_keywords_in_link(i.get('href'), categories):\n","            cate_link = base_link_remove_last_slash + i.get('href')\n","            result.append([remove_first_slash(i.get('href')), cate_link])\n","            print([remove_first_slash(i.get('href')), cate_link])\n","\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"sl8jvT0vhHJL"},"source":["#### Crawl max page of category"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6R7w9fgUhHJM"},"outputs":[],"source":["def get_end_page_from_category(url):\n","    #first find url\n","    driver = executing_selenium(url)\n","    category_url = \"\"\n","    try:\n","        driver.find_element(By.CLASS_NAME,'view-more')\n","        driver.find_element(By.CLASS_NAME,'view-more > a').click()\n","        time.sleep(2)\n","        category_url = driver.current_url\n","    except:\n","        category_url = url\n","\n","    pre, end = split_url(category_url)\n","    if pre != \"\" and end != \"\":\n","        stop = False\n","        end_url = pre\n","        count = int(end)\n","        while stop == False:\n","            driver.get(end_url + str(count))\n","            try:\n","                driver.find_element(By.CLASS_NAME, 'view-more')\n","                driver.find_element(By.CLASS_NAME,'view-more > a').click()\n","                time.sleep(2)\n","                count += 1\n","                end_url = driver.current_url\n","            except:\n","                stop = True\n","\n","        end = count\n","        print(pre + str(end))\n","    return pre + str(end)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9sSP9hS2hHJM"},"outputs":[],"source":["def crawl_list(base_link, url):\n","    driver =  executing_selenium(url)\n","    content = driver.page_source\n","    remove_key_words = ['javascript:void(0)','#']\n","    list_href = []\n","    soup = BeautifulSoup(content, \"html.parser\")\n","\n","    list_products = soup.find('ul', class_='listproduct')\n","    base_link_remove_last_slash = remove_last_slash(base_link)\n","    if list_products != None:\n","        li_products = list_products.findChildren('li')\n","        for li in li_products:\n","            a_products = li.findChildren('a')\n","            for a in a_products:\n","                if not any (word in a.get('href') for word in remove_key_words):\n","                    product_url = base_link_remove_last_slash + a.get('href')\n","                    list_href.append([a.get(\"data-name\"), a.get('href'), product_url])\n","\n","        print(f\"Get {len(li_products)} from {url}\")\n","\n","    return list_href"]},{"cell_type":"markdown","metadata":{"id":"qDdxsOi1hHJM"},"source":["#### Save Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3S-By4LhHJM"},"outputs":[],"source":["# def write_to_csv(data, path, column_names, index = True, append=False):\n","#     if data == None:\n","#         return \"Data is null\"\n","#     if index == True:\n","#         #Đánh index cho mỗi record\n","#         data = [(i+1, item) for i, item in enumerate(data)]\n","#     # check xem có phải là chế độ ghi liên tiếp hay không\n","#     if append:\n","#         # Tồn tại file đó hay không\n","#         if not os.path.exists(path):\n","#             # Nếu tập tin chưa tồn tại, tạo DataFrame từ dữ liệu và ghi vào tập tin\n","#             df = pd.DataFrame(data, columns=column_names)\n","#             df.to_csv(path, index=False)\n","#             return \"Data has written to csv file\"\n","#         df = pd.DataFrame(data)\n","#         df.to_csv(path, mode='a', header=False, index=False)\n","\n","#     else:\n","#         if not os.path.exists(path):\n","#             # Nếu tập tin chưa tồn tại, tạo DataFrame từ dữ liệu và ghi vào tập tin\n","#             df = pd.DataFrame(data, columns=column_names)\n","#             df.to_csv(path, index=False)\n","#             return \"Data has written to csv file\"\n","#         else:\n","#             os.remove(path)\n","#             print (\"Found a csv file, deleting it...\")\n","#             df = pd.DataFrame(data, columns=column_names)\n","#             df.to_csv(path, index=False)\n","#             return \"Data has written to csv file\"\n","\n","\n","import os\n","import pandas as pd\n","\n","def write_to_csv(data, path, column_names, index=True, append=False):\n","    if data is None or len(data) == 0:\n","        return \"Data is null or empty\"\n","\n","    # Gắn index nếu cần\n","    if index:\n","        data = [(i + 1, *item) for i, item in enumerate(data)]\n","        column_names = [\"index\"] + column_names\n","\n","    # Chuyển data thành DataFrame\n","    df = pd.DataFrame(data, columns=column_names)\n","\n","    # Ghi tiếp vào file\n","    if append and os.path.exists(path):\n","        df.to_csv(path, mode='a', header=False, index=False)\n","        return f\"Appended {len(df)} rows to {path}\"\n","    else:\n","        # Ghi đè (hoặc lần đầu tạo)\n","        df.to_csv(path, index=False)\n","        return f\"Wrote {len(df)} rows to {path}\"\n"]},{"cell_type":"markdown","metadata":{"id":"1CEhNJChhHJM"},"source":["#### Read Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAvzmxIqhHJN"},"outputs":[],"source":["def read_csv(path, is_read_header = True):\n","    data = []\n","    if not os.path.exists(path):\n","        print(f\"File '{path}' does not exist.\")\n","        return data\n","    # Mở tập tin CSV để đọc\n","    with open(path, 'r', newline='',encoding='utf-8') as file:\n","        reader = csv.reader(file)\n","        if is_read_header:\n","            next(reader)\n","        # Duyệt qua từng hàng trong tập tin CSV\n","        for row in reader:\n","            data.append(row)\n","    return data"]},{"cell_type":"markdown","metadata":{"id":"zmY4zeA7hHJN"},"source":["#### Main\n","##### 1. Crawl all category of thegioididong and save to csv file\n","##### 2. Crawl all end page (find button view-more at the end of the page')\n","##### 3. Crawl all product link from end page link"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0FTvEB5hHJN","outputId":"3121df13-6a93-440e-f761-5429a3c6eb33"},"outputs":[{"name":"stdout","output_type":"stream","text":["['dtdd', 'https://www.thegioididong.com/dtdd']\n","['sac-dtdd', 'https://www.thegioididong.com/sac-dtdd']\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_7404\\2867265181.py:7: DeprecationWarning: Call to deprecated method findChildren. (Replaced by find_all) -- Deprecated since version 3.0.0.\n","  list_a =  menu.findChildren('a')\n"]},{"data":{"text/plain":["'Wrote 2 rows to tgdt_categories.csv'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["base_link = \"https://www.thegioididong.com/\"\n","\n","removing_keyword = ['op-lung', 'mieng-dan', 'ldp', 'tien-ich/', 'gia-do-dien-thoai', 'tui-dung-airpods', 'tui-chong-soc', 'sim-so-dep']\n","categories = ['dtdd'] # , 'may-tinh-bang', 'laptop', 'phu-kien', 'dong-ho-thong-minh', 'phu-kien-dien-thoai', 'phu-kien-may-tinh-bang', 'phu-kien-laptop']\n","# # Category crawl\n","categories = crawl_category(base_link, categories,removing_keyword)\n","write_to_csv(categories, \"tgdt_categories.csv\",[\"category\", \"full link\"], False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dW0mvZhDhHJN","outputId":"8836b444-39a9-463f-affc-2314985fe25f"},"outputs":[{"name":"stdout","output_type":"stream","text":["https://www.thegioididong.com/dtdd#c=42&o=13&pi=6\n","https://www.thegioididong.com/sac-dtdd#c=57&o=13&pi=3\n","Wrote 2 rows to tgdd_end_page_link.csv\n"]}],"source":["# end_link crawl\n","end_page_link = []\n","for i in  read_csv(\"tgdt_categories.csv\", True):\n","    end_page_link.append([i[0], get_end_page_from_category(i[1])])\n","print(write_to_csv(end_page_link, \"tgdd_end_page_link.csv\", [\"category\", \"full link\"], False))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxGxTUT0hHJN","outputId":"364363f6-8383-49eb-a85a-27e682cb4b71"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_7404\\337275777.py:11: DeprecationWarning: Call to deprecated method findChildren. (Replaced by find_all) -- Deprecated since version 3.0.0.\n","  li_products = list_products.findChildren('li')\n","C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_7404\\337275777.py:13: DeprecationWarning: Call to deprecated method findChildren. (Replaced by find_all) -- Deprecated since version 3.0.0.\n","  a_products = li.findChildren('a')\n"]},{"name":"stdout","output_type":"stream","text":["Get 292 from https://www.thegioididong.com/dtdd#c=42&o=13&pi=6\n","Get 80 from https://www.thegioididong.com/sac-dtdd#c=57&o=13&pi=3\n"]}],"source":["# # product link crawl\n","for i in read_csv(\"tgdd_end_page_link.csv\", True):\n","     data = crawl_list(base_link, i[1])\n","     write_to_csv(data, \"tgdd_product.csv\", [\"name\", \"category\", \"Link\"], False, True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tBNY1ayhHJN"},"outputs":[],"source":["pattern = '[^a-z0-9A-Z_ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂẠẢẤẦẨẪẬẮẰẲẴẶẸẺẼỀỀỂưăạảấầẩẫậắằẳẵặẹẻẽềềểỄỆỈỊỌỎỐỒỔỖỘỚỜỞỠỢỤỦỨỪễếệỉịọỏốồổỗộớờởỡợụủứừỬỮỰỲỴÝỶỸửữựỳýỵỷỹ]+'\n","def preprocess_comment(comment):\n","    comment = comment.strip()\n","    comment = comment.replace('  ', ' ');\n","    return re.sub(pattern,' ', comment).lower()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"phhRB8uPhHJO"},"outputs":[],"source":["def crawl_comment(base_url, category, product):\n","    data = []\n","    count = 0\n","\n","    url = base_url + category + '/' + product\n","    print(url)\n","    driver = executing_selenium(url)\n","    time.sleep(1)\n","    try:\n","        driver.find_element(By.CLASS_NAME, 'btn-view-all')\n","        url = base_url + category + '/' + product + '/danh-gia?page='\n","        number_page = 1\n","        while True:\n","            driver = executing_selenium(url + str(number_page))\n","            time.sleep(1)\n","            content = driver.page_source\n","            soup = BeautifulSoup(content, \"html.parser\")\n","            ul_comments = soup.find('ul', class_='comment-list')\n","            if ul_comments is None:\n","                print(f\"Crawl {base_url + category + '/' + product } done!! Found {count} comments\")\n","                break\n","            li_comments = ul_comments.find_all('li', {\"class\" : \"par\"})\n","            for li_comment in li_comments:\n","                div_comment = li_comment.find('div', {\"class\": \"cmt-content\"})\n","                start_buy = li_comment.find_all('i', {'class' : 'iconcmt-starbuy'})\n","                comment = preprocess_comment(div_comment.text)\n","                if comment != \"\":\n","                    data.append([category, product, len(start_buy), comment])\n","                    count += 1\n","            # div_comments = ul_comments.find_all('div', {\"class\": \"cmt-content\"})\n","            # for div_comment in div_comments:\n","            #     comment = preprocess_comment(div_comment.text)\n","            #     if comment != \"\":\n","            #         data.append([category, product, comment])\n","            #         count += 1\n","            number_page = number_page + 1\n","    except:\n","        content = driver.page_source\n","        soup = BeautifulSoup(content, \"html.parser\")\n","        ul_comments = soup.find('ul', class_='comment-list')\n","        if ul_comments is None:\n","            print(f\"Crawl {base_url + category + '/' + product } done!! Found {count} comments\")\n","        else:\n","            # div_comments = ul_comments.find_all('p', {\"class\": \"cmt-txt\"})\n","            # for div_comment in div_comments:\n","            #     comment = preprocess_comment(div_comment.text)\n","            #     if comment != \"\":\n","            #         data.append([category, product, comment])\n","            #         count += 1\n","            li_comments = ul_comments.find_all('li', class_='par')\n","            for li_comment in li_comments:\n","                div_comment = li_comment.find('div', {\"class\": \"cmt-tx\"})\n","                start_buy = li_comment.find_all('i', {'class' : 'iconcmt-starbuy'})\n","                comment = preprocess_comment(div_comment.text)\n","                if comment != \"\":\n","                    data.append([category, product, len(start_buy), comment])\n","                    count += 1\n","\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBTEPRJPhHJO","outputId":"f574d024-22d1-4393-c2e7-d08f4fe0d25a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Đã tạo file product.csv thành công.\n","https://www.thegioididong.com/dtdd/samsung-galaxy-s24-ultra-5g\n","Crawl https://www.thegioididong.com/dtdd/samsung-galaxy-s24-ultra-5g done!! Found 130 comments\n"]},{"data":{"text/plain":["'Appended 130 rows to product.csv'"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["with open('product.csv', mode='w', newline='', encoding='utf-8') as file:\n","    fieldnames = [\"category\", \"product\", \"star\", 'comments']\n","    writer = csv.DictWriter(file, fieldnames=fieldnames)\n","\n","    writer.writeheader()\n","print(\"Đã tạo file product.csv thành công.\")\n","products = read_csv('product.csv', True)\n","for i in products:\n","    category, product_name = extract_path_and_remove_query_params(i[1])\n","    comments = crawl_comment(base_link, category, product_name)\n","    write_to_csv(comments, \"tgdd_comments.csv\", [\"category\", \"product\", \"star\",'comments'], False, True)\n","comments = crawl_comment(base_link, \"dtdd\", \"samsung-galaxy-s24-ultra-5g\")\n","write_to_csv(comments, \"product.csv\", [\"category\", \"product\",\"star\", 'comments'], False, True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9G8SWRbhHJO","outputId":"62b5f385-9035-4470-a042-46bac21bf982"},"outputs":[{"name":"stdout","output_type":"stream","text":["không tìm thấy sản phẩm\n"]},{"data":{"text/plain":["[]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# Nếu có lỗi trong quá trình crawl thì chạy hàm này\n","def crawl_product_continue(current_product):\n","    product_list = read_csv('product.csv', True)\n","    df = pd.read_csv('product.csv')\n","    product_category = df[\"category\"].tolist()\n","    try:\n","        target_index = product_category.index(current_product)\n","        products_after_target = product_list[target_index:]\n","        for i in products_after_target:\n","            category_continue, product_name_continue = extract_path_and_remove_query_params(i[1])\n","            comments = crawl_comment(base_link, category_continue, product_name_continue)\n","            write_to_csv(comments, \"tgdd_comments.csv\", [\"category\", \"product\", \"star\",'comments'], False, True)\n","        return products_after_target\n","    except ValueError:\n","        print(\"không tìm thấy sản phẩm\")\n","        return []\n","\n","crawl_product_continue(\"/phan-mem/eset-nod32-antivirus-cho-windows-1-pc\")"]}],"metadata":{"kernelspec":{"display_name":"thac123 (3.13.3)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}